---
title: "Two-layer networks with the $\text{ReLU}^{k}$ activation function: Barron spaces and derivative approximation"
collection: publications
category: manuscripts
permalink: /publication/2023-11-23-ExtendedBarron
excerpt: <!-- 'This paper is about the number 1. The number 2 is left for future work.' -->
date: 2023-11-23
venue: 'Numerische Mathematik'
slidesurl: <!-- 'http://academicpages.github.io/files/slides1.pdf' -->
paperurl: <!-- 'http://academicpages.github.io/files/paper1.pdf' -->
citation: <!-- 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).' -->
---

We investigate the use of two-layer networks with the rectified power unit, which is called the $\text{ReLU}^{k}$ activation function, for function and derivative approximation. By extending and calibrating the corresponding Barron space, we show that two-layer networks with the $\text{ReLU}^{k}$ activation function are well-designed to simultaneously approximate an unknown function and its derivatives. When the measurement is noisy, we propose a Tikhonov type regularization method, and provide error bounds when the regularization parameter is chosen appropriately. Several numerical examples support the efficiency of the proposed approach.
